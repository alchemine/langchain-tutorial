{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 28, 'total_tokens': 33}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-9b0f47fd-0ad8-4e1a-b233-f62e4e4fb273-0', usage_metadata={'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from core.llm import CHAT_LLM as model\n",
    "\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(\"My name is Bob.\"),\n",
    "        AIMessage(\"Hello Bob!\"),\n",
    "        HumanMessage(\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Bob.\n",
      "{'abc2': InMemoryChatMessageHistory(messages=[HumanMessage(content='My name is Bob.'), AIMessage(content='Hello Bob!'), HumanMessage(content=\"What's my name?\"), AIMessage(content='Your name is Bob.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 28, 'total_tokens': 33}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-0b6216c3-1a9c-4c74-be1d-80bdec7763b9-0', usage_metadata={'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33})])}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    model,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(\"My name is Bob.\"),\n",
    "        AIMessage(\"Hello Bob!\"),\n",
    "        HumanMessage(\"What's my name?\"),\n",
    "    ],\n",
    "    config,\n",
    ")\n",
    "\n",
    "print(response.content)\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain with history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from core.llm import CHAT_LLM as model\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all questions.\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 25, 'total_tokens': 39}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-48f0b707-587a-42dc-83df-c75379da8184-0', usage_metadata={'input_tokens': 25, 'output_tokens': 14, 'total_tokens': 39})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"My name is Bob\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 52, 'total_tokens': 57}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-7712dc12-6b3b-421a-96ce-936a1d0960a9-0', usage_metadata={'input_tokens': 52, 'output_tokens': 5, 'total_tokens': 57})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What's my name?\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Conversation History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_core.messages import SystemMessage, trim_messages, BaseMessage\n",
    "\n",
    "from core.llm import CHAT_LLM\n",
    "\n",
    "\n",
    "def token_counter(\n",
    "    messages: list[BaseMessage], model_name: str = CHAT_LLM.model_name\n",
    ") -> int:\n",
    "    \"\"\"Token counter for LangChain BaseMessage objects.\n",
    "\n",
    "    Args:\n",
    "        messages (List[BaseMessage]): List of LangChain BaseMessage objects.\n",
    "        model_name (str): Name of the model to use for token encoding.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of tokens in the messages.\n",
    "\n",
    "    Note:\n",
    "        This function uses the tiktoken library for accurate token counting.\n",
    "        It handles both string and list content types in BaseMessage objects.\n",
    "        This doesn't account for non-text content like images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "    except KeyError:\n",
    "        print(f\"Warning: model {model_name} not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        # Every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        num_tokens += 4\n",
    "\n",
    "        # print(\"message:\", message)\n",
    "        for key, value in message.dict().items():\n",
    "            # print(f\" - {key}: {value}\")\n",
    "            if key == \"content\":\n",
    "                if isinstance(value, str):\n",
    "                    num_tokens += len(encoding.encode(value))\n",
    "                elif isinstance(value, list):\n",
    "                    for content_item in value:\n",
    "                        if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                            num_tokens += len(encoding.encode(content_item[\"text\"]))\n",
    "            elif key == \"role\":\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "        # print(f\" - num_tokens: {num_tokens+2}\")\n",
    "\n",
    "    num_tokens += 2  # Every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    # token_counter=model,\n",
    "    token_counter=token_counter,  # fallback method\n",
    "    include_system=True,\n",
    "    start_on=\"human\",\n",
    "    end_on=\"ai\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm always here to help you out.\" response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 73, 'total_tokens': 82}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}} id='run-b05b3e40-0a13-4e1c-a9ce-b8c558ca7e7e-0' usage_metadata={'input_tokens': 73, 'output_tokens': 9, 'total_tokens': 82}\n",
      "I'm always here to help you out.\n",
      "{'input_tokens': 73, 'output_tokens': 9, 'total_tokens': 82}\n"
     ]
    }
   ],
   "source": [
    "rst = model.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"you're a good assistant\"),\n",
    "        AIMessage(content=\"hi!\"),\n",
    "        HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "        AIMessage(content=\"nice\"),\n",
    "        HumanMessage(content=\"whats 2 + 2\"),\n",
    "        AIMessage(content=\"4\"),\n",
    "        HumanMessage(content=\"thanks\"),\n",
    "        AIMessage(content=\"no problem!\"),\n",
    "        HumanMessage(content=\"having fun?\"),\n",
    "        AIMessage(content=\"yes!\"),\n",
    "    ]\n",
    ")\n",
    "print(rst)\n",
    "print(rst.content)\n",
    "print(rst.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='How can I assist you today, Bob?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 72, 'total_tokens': 81}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-b8071458-2f5f-41fc-8088-02e8a2b145cc-0', usage_metadata={'input_tokens': 72, 'output_tokens': 9, 'total_tokens': 81})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"My name is Bob\")]},\n",
    "    config=config,\n",
    ")\n",
    "with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(\"What's my name?\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "- Details: https://python.langchain.com/v0.2/docs/how_to/streaming/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|I|'m| sorry|,| I| don|'t| have| access| to| your| personal| information|.| How| can| I| assist| you| today|?||"
     ]
    }
   ],
   "source": [
    "for r in with_message_history.stream(\n",
    "    {\"messages\": [HumanMessage(\"My name is Bob\")]},\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial-9TtSrW0h-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
